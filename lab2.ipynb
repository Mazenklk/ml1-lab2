{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Lab 2 - Linear regression </br> M1 Data Science, ML1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# \"IPython magic command\" to sutomatically reload the imported packages after changes in a package\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "# plt.rc(\"font\", **{\"family\": \"sans-serif\", \"sans-serif\": [\"Helvetica\"]})\n",
    "# plt.rc(\"text\", usetex=True)\n",
    "\n",
    "SMALL_SIZE = 16\n",
    "MEDIUM_SIZE = 20\n",
    "BIGGER_SIZE = 24\n",
    "\n",
    "plt.rc(\"font\", size=SMALL_SIZE)  # default text size\n",
    "plt.rc(\"axes\", titlesize=SMALL_SIZE)  # fontsize of the axes title\n",
    "plt.rc(\"axes\", labelsize=MEDIUM_SIZE)  # fontsize of the x and y labels\n",
    "plt.rc(\"xtick\", labelsize=SMALL_SIZE)  # fontsize of the tick labels\n",
    "plt.rc(\"ytick\", labelsize=SMALL_SIZE)  # fontsize of the tick labels\n",
    "plt.rc(\"legend\", fontsize=SMALL_SIZE)  # legend fontsize\n",
    "plt.rc(\"figure\", titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "# display backend for matplotlib\n",
    "%matplotlib inline\n",
    "# %matplotlib widget\n",
    "# %matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\" style=\"margin-top: 0px\">\n",
    "\n",
    "\n",
    "Lab report written by: Kalakech Mazen, Mchawrab Ali 2025-2026.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: linear regression\n",
    "\n",
    "Let $N, M$ be two postive integers, and consider a dataset $\\mathcal{D} = \\{ (y_n \\mathbf{x}_n) \\}_{1 \\leq n \\leq N}$, with targets $\\mathbf{y} = (y_n)_{1 \\leq n \\leq N} \\in \\mathbb{R}^N$, and (standardized) data $\\mathbf{X} = [\\mathbf{x}_1, \\dotsc, \\mathbf{x}_N]^T \\in \\mathbb{R}^{N \\times M}$.\n",
    "\n",
    "This exercise is aimed at performing linear regression with `numpy` and `sklearn` on the `California house` dataset, loaded below.\n",
    "\n",
    "For the sake of simplicity, the dataset is NOT split into a train and a test set until question 5, where the quality of the model is properly assessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640, 8) (20640,)\n",
      "['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n",
      ".. _california_housing_dataset:\n",
      "\n",
      "California Housing dataset\n",
      "--------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 20640\n",
      "\n",
      "    :Number of Attributes: 8 numeric, predictive attributes and the target\n",
      "\n",
      "    :Attribute Information:\n",
      "        - MedInc        median income in block group\n",
      "        - HouseAge      median house age in block group\n",
      "        - AveRooms      average number of rooms per household\n",
      "        - AveBedrms     average number of bedrooms per household\n",
      "        - Population    block group population\n",
      "        - AveOccup      average number of household members\n",
      "        - Latitude      block group latitude\n",
      "        - Longitude     block group longitude\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "This dataset was obtained from the StatLib repository.\n",
      "https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n",
      "\n",
      "The target variable is the median house value for California districts,\n",
      "expressed in hundreds of thousands of dollars ($100,000).\n",
      "\n",
      "This dataset was derived from the 1990 U.S. census, using one row per census\n",
      "block group. A block group is the smallest geographical unit for which the U.S.\n",
      "Census Bureau publishes sample data (a block group typically has a population\n",
      "of 600 to 3,000 people).\n",
      "\n",
      "A household is a group of people residing within a home. Since the average\n",
      "number of rooms and bedrooms in this dataset are provided per household, these\n",
      "columns may take surprisingly large values for block groups with few households\n",
      "and many empty houses, such as vacation resorts.\n",
      "\n",
      "It can be downloaded/loaded using the\n",
      ":func:`sklearn.datasets.fetch_california_housing` function.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
      "      Statistics and Probability Letters, 33 (1997) 291-297\n",
      "\n",
      "[4.526 3.585 3.521 ... 0.923 0.847 0.894]\n"
     ]
    }
   ],
   "source": [
    "# loading the dataset\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "# X = housing.data, y = housing.target\n",
    "X = housing.data\n",
    "y = housing.target\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "print(housing.feature_names)\n",
    "print(housing.DESCR)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranges: \n",
      " names: ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude'] \n",
      " dr.  : [1.45e+01 5.10e+01 1.41e+02 3.37e+01 3.57e+04 1.24e+03 9.41e+00 1.00e+01] \n",
      " min  : [   0.5     1.      0.85    0.33    3.      0.69   32.54 -124.35] \n",
      " max  : [ 1.50e+01  5.20e+01  1.42e+02  3.41e+01  3.57e+04  1.24e+03  4.20e+01\n",
      " -1.14e+02]\n"
     ]
    }
   ],
   "source": [
    "features_min = np.min(X, axis=0)\n",
    "features_max = np.max(X, axis=0)\n",
    "range_features = np.abs(features_max - features_min)\n",
    "\n",
    "with np.printoptions(precision=2):\n",
    "    print(\n",
    "        \"Feature ranges: \\n names: {} \\n dr.  : {} \\n min  : {} \\n max  : {}\".format(\n",
    "            housing.feature_names, range_features, features_min, features_max\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert\" style=\"margin-top: 0px\">\n",
    "\n",
    "1. Standardize the data using `sklearn`, and store the output into a variable `scaled_X`. Is it useful for this dataset? Justify your answer.\n",
    "\n",
    "> Indication: take a look at [`sklearn.preprocessing.StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the Dataset, we can see that there is a big discrepency between the features ranges. This means that the cost function will be dominated by some features\n",
    "import sklearn.preprocessing as pp \n",
    "scaler = pp.StandardScaler()\n",
    "scaled_X = scaler.fit_transform(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranges: \n",
      " names: ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude'] \n",
      " dr.  : [  7.63   4.05  57.02  71.18  31.51 119.65   4.41   5.01] \n",
      " min  : [-1.77 -2.2  -1.85 -1.61 -1.26 -0.23 -1.45 -2.39] \n",
      " max  : [  5.86   1.86  55.16  69.57  30.25 119.42   2.96   2.63]\n"
     ]
    }
   ],
   "source": [
    "features_min = np.min(scaled_X, axis=0)\n",
    "features_max = np.max(scaled_X, axis=0)\n",
    "range_features = np.abs(features_max - features_min)\n",
    "\n",
    "with np.printoptions(precision=2):\n",
    "    print(\n",
    "        \"Feature ranges: \\n names: {} \\n dr.  : {} \\n min  : {} \\n max  : {}\".format(\n",
    "            housing.feature_names, range_features, features_min, features_max\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" style=\"margin-top: 0px\">\n",
    "Answer question 1:\n",
    "\n",
    "...\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert\" style=\"margin-top: 0px\">\n",
    "\n",
    "2. Write ERM problem corresponding to linear least-squares regression. Recall the form of the analytic expression solution, with the definition of the elements involved.\n",
    "\n",
    "    > Indication: type your answer in $\\LaTeX$ in the cell below, or include a picture of your handwritten answer.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert\" style=\"margin-top: 0px\">\n",
    "\n",
    "3. \n",
    "   1. Compute the solution to the problem with `numpy` from the analytic expression, calling the result `beta_numpy`.\n",
    "   \n",
    "   2. Compute the solution with `sklearn`, `beta_sklearn`. \n",
    "\n",
    "   3. Test whether the two array are significantly different. \n",
    "\n",
    "> Hints: \n",
    "> - the function [`numpy.linalg.solve`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html#numpy.linalg.solve) and the class [`sklearn.linear_model.LinearRegression`](https://scikit-learn.org/stable/modules/linear_model.html) will be useful\n",
    "> - the function [`numpy.allclose`](https://numpy.org/doc/stable/reference/generated/numpy.allclose.html#numpy-allclose) can be useful to test equality between two arrays.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.06855817  0.8296193   0.11875165 -0.26552688  0.30569623 -0.004503\n",
      " -0.03932627 -0.89988565 -0.870541  ]\n",
      "[ 2.09471497  0.98418508  0.21359445 -0.44461931  0.46112547  0.05130131\n",
      " -0.02362868 -0.3766683  -0.36058933]\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "# Computing X_tilda and its transpose to use the numpy solution\n",
    "X_tilda = np.c_[np.ones((np.shape(scaled_X)[0],1)),scaled_X]\n",
    "X_tildaT = np.transpose(X_tilda)\n",
    "# Beta* = (X_tildaT * X_tilda)-1 X_tilda y\n",
    "# we are solving this equation using Numpy to find Beta* \n",
    "A = np.matmul(X_tildaT, X_tilda)\n",
    "b = np.matmul(X_tildaT, y)\n",
    "beta_numpy = np.linalg.solve(A, b)\n",
    "print(beta_numpy)\n",
    "## Using sklearn\n",
    "\n",
    "regression = linear_model.LinearRegression()\n",
    "regression.fit(A,b)\n",
    "beta_sklearn=regression.coef_\n",
    "b=regression.intercept_\n",
    "print(beta_sklearn)\n",
    "#comparing the two solutions\n",
    "\n",
    "print(np.allclose(beta_numpy,beta_sklearn))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert\" style=\"margin-top: 0px\">\n",
    "\n",
    "4. Evaluate $\\kappa(\\widetilde{\\mathbf{X}}^T\\widetilde{\\mathbf{X}})$, the condition number of the matrix $\\widetilde{\\mathbf{X}}^T\\widetilde{\\mathbf{X}}$ using the function [`numpy.linalg.cond`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.cond.html#numpy.linalg.cond). Is the least-squares problem considered well-conditioned? Justify your answer.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.46514975130451\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' A = X_tilta*X_tildaT is ill-conditioned because k == 44.4651 >> 1'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = np.linalg.cond(A)\n",
    "print(k)\n",
    "''' A = X_tilta*X_tildaT is ill-conditioned because k == 44.4651 >> 1'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" style=\"margin-top: 0px\">\n",
    "Answer question 4:\n",
    "\n",
    "...\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert\" style=\"margin-top: 0px\">\n",
    "\n",
    "5. To properly assess the quality of the regressor, the dataset will now be split into a train and a test set. Complete the code below to:\n",
    "    - split the dataset into a train and a test set\n",
    "    - use `sklearn` to apply K-fold cross validation on the train set, and return the regressor associated with the best MSE criterion;\n",
    "    - evaluate the performance on the test set in terms of the MSE criterion.\n",
    "\n",
    "    > Indications: \n",
    "    > - if needed, take a look at the documentation of [`sklearn.model_selection.train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#train-test-split), and take some inspiration from [`sklearn` examples](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html#sphx-glr-auto-examples-linear-model-plot-ols-py).\n",
    "    >\n",
    "    > - the documentation and examples for [`Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) (or [`sklearn.pipeline.make_pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html)) and [`sklearn.preprocessing.StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) can be useful;\n",
    "    >\n",
    "    > - to return the regression coefficients from the pipeline, take a look its [named_steps](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline.named_steps) method (if needed, see also the [stackoverflow answer](https://stackoverflow.com/questions/43856280/return-coefficients-from-pipeline-object-in-sklearn));\n",
    "    >\n",
    "    > - for the evaluation metrics, see [the documentation](https://scikit-learn.org/stable/modules/model_evaluation.html) and the module [sklearn.metrics](https://scikit-learn.org/stable/api/sklearn.metrics.html) \n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m kf \u001b[38;5;241m=\u001b[39m KFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     24\u001b[0m fold_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_index, test_index \u001b[38;5;129;01min\u001b[39;00m kf\u001b[38;5;241m.\u001b[39msplit(\u001b[43mX_train\u001b[49m):\n\u001b[0;32m     26\u001b[0m     X_subtrain, X_subtest \u001b[38;5;241m=\u001b[39m X_train[train_index], X_train[test_index]\n\u001b[0;32m     27\u001b[0m     y_subtrain, y_subtest \u001b[38;5;241m=\u001b[39m y_train[train_index], y_train[test_index]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# TODO: instructions below to be completed\n",
    "# X_train, X_test, y_train, y_test = ...\n",
    "\n",
    "\n",
    "# TODO: instructions below to be completed\n",
    "# Create a linear regression model\n",
    "# linear_regression = ...\n",
    "\n",
    "# Create scaler\n",
    "# scaler = ...\n",
    "\n",
    "# TODO: uncomment once code for linear_regression and scaler variables are filled-in\n",
    "# pipe = Pipeline(steps=[(\"scaler\", scaler), (\"linearregression\", linear_regression)])\n",
    "\n",
    "# Set-up cross-validation\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "fold_id = 0\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    X_subtrain, X_subtest = X_train[train_index], X_train[test_index]\n",
    "    y_subtrain, y_subtest = y_train[train_index], y_train[test_index]\n",
    "\n",
    "    # train the model using the scaled train set\n",
    "    # TODO: add missing instruction\n",
    "    # ...\n",
    "\n",
    "    # make predictions using the test set\n",
    "    # TODO: add missing instruction, using the pipe object\n",
    "    # y_pred = ...\n",
    "\n",
    "    # TODO: compute the errors (MSE, MAE, R2), and display these below\n",
    "    # display mean squared error\n",
    "    print(\"Fold {}: MSE: {:.2f}\".format(fold_id, 0))\n",
    "    # display mean absolute error\n",
    "    print(\"        MAE: {:.2f}\".format(0))\n",
    "    # display coefficient of determination: 1 is perfect prediction\n",
    "    print(\"        R2 : {:.2f}\".format(0))\n",
    "    fold_id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: least-squares and matrix inversion\n",
    "\n",
    "Let $N, M$ be two positive integers, $\\mathbf{A} \\in \\mathbb{R}^{N \\times M}$ such that $\\text{rank}(\\mathbf{A}) = M$, and $\\mathbf{y} \\in \\mathbb{R}^N$. \n",
    "\n",
    "This exercise is aimed at comparing the accuracy and speed of 2 approaches to compute $\\widehat{\\boldsymbol{\\beta}} = (\\mathbf{A}^T\\mathbf{A})^{-1}\\mathbf{A}^T\\mathbf{y}$, solution to the problem \n",
    "\n",
    "\\begin{equation*}\n",
    "    \\underset{\\boldsymbol{\\beta} \\in \\mathbb{R}^M}{\\text{minimize}} \\; \\frac{1}{2} \\| \\mathbf{y} - \\mathbf{A}\\boldsymbol{\\beta} \\|_2^2\n",
    "\\end{equation*}\n",
    "\n",
    "1. computing $\\mathbf{B} = (\\mathbf{A}^T\\mathbf{A})^{-1}$, and then $\\widehat{\\boldsymbol{\\beta}} = \\mathbf{B}\\mathbf{A}^T \\mathbf{y}$.\n",
    "2. solving the problem with an iterative solver, e.g., using [`numpy.linalg.solve`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html#numpy.linalg.solve).\n",
    "\n",
    "To do so, we will use the data generated below, knowing the ground truth value $\\boldsymbol{\\beta}^*$ to evaluate numerical accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1234)\n",
    "\n",
    "N = 200\n",
    "M = 100\n",
    "\n",
    "A = rng.standard_normal(size=(N, M))\n",
    "beta_true = rng.standard_normal(size=(M,))\n",
    "y = A @ beta_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert\" style=\"margin-top: 0px\">\n",
    "\n",
    "1. \n",
    "   - Compute $\\widehat{\\boldsymbol{\\beta}}$ with the two approaches mentioned above, registering the time needed to solve the problem with the 2 version. To do so, complete the code cell below.\n",
    "   - Compare the time required between the two approaches\n",
    "\n",
    "> Indication: see the functions [`np.linalg.inv`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.inv.html#numpy.linalg.inv) and [`numpy.linalg.solve`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html#numpy.linalg.solve)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by algo 1: 0.002269299999170471\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "start1 = timeit.timeit()\n",
    "B = np.linalg.inv(np.matmul(np.transpose(A),A))\n",
    "beta_inv = np.matmul(B,np.transpose(A)).dot(y)\n",
    "end1 = timeit.timeit()\n",
    "algo1_time = np.abs(start1 - end1)\n",
    "print(f\"Time taken by algo 1: {algo1_time}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by algo 2: 0.0010371999997005332\n",
      "Second approach is faster\n"
     ]
    }
   ],
   "source": [
    "start2 = timeit.timeit()\n",
    "beta_solv = np.linalg.solve(np.matmul(np.transpose(A),A),np.matmul(np.transpose(A),y))\n",
    "end2 = timeit.timeit()\n",
    "algo2_time = np.abs(start2 - end2)\n",
    "print(f\"Time taken by algo 2: {algo2_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second approach is faster\n",
      "45.705724235653086\n"
     ]
    }
   ],
   "source": [
    "if algo1_time < algo2_time:\n",
    "    print(\"First approach is faster\")\n",
    "else:\n",
    "    print(\"Second approach is faster\")\n",
    "'''Result: method 2 is faster'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert\" style=\"margin-top: 0px\">\n",
    "\n",
    "2. The accuracy of $\\widehat{\\boldsymbol{\\beta}}$ can be assess with respect to the ground truth $\\boldsymbol{\\beta}^*$ in terms of the reconstruction signal to noise ratio (rSNR), expressed in dB, defined by\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\text{rSNR}(\\widehat{\\boldsymbol{\\beta}}, \\boldsymbol{\\beta}^*) = 10 \\log_{10} \\Big( \\frac{\\| \\boldsymbol{\\beta}^* \\|_2^2}{\\| \\boldsymbol{\\beta}^* - \\widehat{\\boldsymbol{\\beta}} \\|_2^2} \\Big).\n",
    "\\end{equation*}\n",
    "\n",
    "- Compute the rSNR for the solution `beta_inv` and `beta_solve` computed above, and indicate which of the two esimates is the most precise.\n",
    "- Conclude on the best approach to adopt to solve linear systems of equations.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rSNR of beta_inv = 296.7504037649167\n",
      "rSNR of beta_solv = 296.60295904998856\n",
      "first approach is more accurate\n",
      "ratio of algo2_time over algo1_time : 45.705724235653086\n",
      "ratio of rSNR_Algo1 over rSNR_Algo2 : 99.9503135587829\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Conclusion: the rSNR of the first method is higher than the second one by'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rSNR_Algo1 = 10*np.log10(np.square(np.linalg.norm(beta_true))/np.square(np.linalg.norm(beta_true - beta_inv)))\n",
    "print(f\"rSNR of beta_inv = {rSNR_Algo1}\")\n",
    "rSNR_Algo2 = 10*np.log10(np.square(np.linalg.norm(beta_true))/np.square(np.linalg.norm(beta_true - beta_solv)))\n",
    "print(f\"rSNR of beta_solv = {rSNR_Algo2}\")\n",
    "if rSNR_Algo2 < rSNR_Algo1:\n",
    "    print(\"first approach is more accurate\")\n",
    "else:\n",
    "    print(\"second approach is more accurate\")\n",
    "    \n",
    "'''We want to compute the ratio of the two methods execution time'''\n",
    "print(f\"ratio of algo2_time over algo1_time : {(algo2_time/algo1_time) * 100}\")\n",
    "'''We want to compute the ratio of the two methods rSNRs'''\n",
    "print(f\"ratio of rSNR_Algo1 over rSNR_Algo2 : {(rSNR_Algo2/rSNR_Algo1) * 100}\")\n",
    "'''Conclusion: The second method is almost two times (45%) faster than the first one and 99.95% accurate. \n",
    "So for larger dimensions, one may use the second method for faster execution time, but for lower dimensions, method 1 is more useful'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" style=\"margin-top: 0px\">\n",
    "Answer question 2:\n",
    "\n",
    "...\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert\" style=\"margin-top: 0px\">\n",
    "\n",
    "3. Use `numpy` to compute the condition number $\\kappa(\\mathbf{A})$ of the matrix $\\mathbf{A}$. What information does this value convey about the inversion problem considered?\n",
    "\n",
    "> Indication: take a look at the documentation of the function [`numpy.linalg.cond`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.cond.html#numpy.linalg.cond).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.338572426355578\n"
     ]
    }
   ],
   "source": [
    "k = np.linalg.cond(A)\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" style=\"margin-top: 0px\">\n",
    "Answer question 3:\n",
    "\n",
    "...\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "End of lab2."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-research-3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "title": "TP1 TSI : Introduction au filtrage numérique"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
